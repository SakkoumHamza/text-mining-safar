// Assurez-vous que ce package correspond Ã  votre projet
package com.hamza;

import com.hamza.nlp.TfIdfUtils;
import safar.basic.morphology.stemmer.factory.StemmerFactory;
import safar.basic.morphology.stemmer.interfaces.IStemmer;
import safar.basic.morphology.stemmer.model.StemmerAnalysis;
import safar.basic.morphology.stemmer.model.WordStemmerAnalysis;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.function.Function;
import java.util.stream.Collectors;

public class TfIdfProcessor {

    // --- CHEMINS DE CONFIGURATION ---
    private static final String STOP_WORDS_PATH = "/Users/mac/Documents/Projects/TextMinig/src/main/resources/stop_words_arabic.txt";
    private static final String OUTPUT_JSON_PATH = "/Users/mac/Documents/Projects/TextMinig/target/tfidf_results.json";
    private static final List<String> DOCUMENT_PATHS = List.of(
            "/Users/mac/Documents/Projects/TextMinig/src/main/resources/doc1.txt",
            "/Users/mac/Documents/Projects/TextMinig/src/main/resources/doc2.txt",
            "/Users/mac/Documents/Projects/TextMinig/src/main/resources/doc3.txt"
    );

    public static void main(String[] args) throws IOException {
        System.out.println("ðŸš€ DÃ©marrage du pipeline TF-IDF modulaire...");

        // --- 1. Chargement ---
        System.out.println("[Ã‰TAPE 1/6] Chargement des donnÃ©es...");
        Set<String> stopWords = loadStopWords(STOP_WORDS_PATH);
        Map<String, String> documents = loadDocuments(DOCUMENT_PATHS);

        // --- 2. Initialisation ---
        System.out.println("\n[Ã‰TAPE 2/6] Initialisation du Stemmer SAFAR...");
        IStemmer stemmer = initializeStemmer();

        // --- 3. Traitement & Comptage ---
        System.out.println("\n[Ã‰TAPE 3/6] Traitement du corpus...");
        Map<String, Map<String, Long>> occurrenceMap = buildOccurrenceMap(documents, stemmer, stopWords);

        // --- 4. Calcul TF & IDF (avec aperÃ§us) ---
        System.out.println("\n[Ã‰TAPE 4/6] Calcul des scores TF et IDF...");
        Map<String, Map<String, Double>> tfMap = computeTfMap(occurrenceMap);
        // APPEL Ã€ LA CLASSE UTILS
        Map<String, Double> idfMap = TfIdfUtils.computeIDF(occurrenceMap);
        printTfIdfApercus(tfMap, idfMap);

        // --- 5. Calcul TF-IDF Final ---
        System.out.println("\n[Ã‰TAPE 5/6] Calcul des scores TF-IDF finaux...");
        // APPEL Ã€ LA CLASSE UTILS
        Map<String, Map<String, Double>> tfIdfMap = TfIdfUtils.computeTfIdf(tfMap, idfMap);
        System.out.println("âœ… Calcul TF-IDF terminÃ©.");

        // --- 6. Sauvegarde ---
        System.out.println("\n[Ã‰TAPE 6/6] Sauvegarde des rÃ©sultats...");
        // APPEL Ã€ LA CLASSE UTILS
        saveResultsAsJson(tfIdfMap, OUTPUT_JSON_PATH);

        System.out.println("\n--- PIPELINE TERMINÃ‰ ---");
    }

    // =================================================================
    // --- MODULES DU PIPELINE (Logique d'orchestration) ---
    // =================================================================

    /**
     * Ã‰TAPE 1: Charge le fichier de mots vides (stop words).
     */
    private static Set<String> loadStopWords(String path) throws IOException {
        System.out.println("  -> Chargement des mots vides depuis: " + path);
        Set<String> stopWords = Files.lines(Paths.get(path))
                .map(String::trim)
                .filter(line -> !line.isEmpty())
                .collect(Collectors.toSet());
        System.out.println("  âœ… " + stopWords.size() + " mots vides chargÃ©s.");
        return stopWords;
    }

    /**
     * Ã‰TAPE 1: Charge les documents texte depuis une liste de chemins.
     */
    private static Map<String, String> loadDocuments(List<String> filePaths) throws IOException {
        Map<String, String> documents = new LinkedHashMap<>();
        for (int i = 0; i < filePaths.size(); i++) {
            String docName = "doc" + (i + 1);
            String path = filePaths.get(i);
            documents.put(docName, Files.readString(Paths.get(path), StandardCharsets.UTF_8));
            System.out.println("  -> Document '" + docName + "' chargÃ© (" + path + ")");
        }
        return documents;
    }

    /**
     * Ã‰TAPE 2: Initialise le stemmer SAFAR.
     */
    private static IStemmer initializeStemmer() {
        IStemmer stemmer = StemmerFactory.getLight10Implementation();
        System.out.println("  âœ… Raciniseur (Light10) initialisÃ© !");
        return stemmer;
    }

    /**
     * Ã‰TAPE 3: Construit la carte de frÃ©quence des termes (racines) pour tous les documents.
     */
    private static Map<String, Map<String, Long>> buildOccurrenceMap(
            Map<String, String> documents,
            IStemmer stemmer,
            Set<String> stopWords) {

        Map<String, Map<String, Long>> occurrenceMap = new LinkedHashMap<>();

        for (var docEntry : documents.entrySet()) {
            String docName = docEntry.getKey();
            String docText = docEntry.getValue();
            System.out.println("\n  --- Traitement de '" + docName + "' ---");

            List<String> docStems = processText(docText, stemmer, stopWords);

            Set<String> uniqueStems = new HashSet<>(docStems);
            System.out.println("  3. Racines (stems) uniques trouvÃ©es: " + uniqueStems.size());
            System.out.print("    [ ");
            uniqueStems.forEach(s -> System.out.print(s + " "));
            System.out.println("]");

            Map<String, Long> freqMap = docStems.stream()
                    .collect(Collectors.groupingBy(Function.identity(), Collectors.counting()));

            occurrenceMap.put(docName, freqMap);
            System.out.println("  âœ… '" + docName + "' traitÃ©.");
        }
        return occurrenceMap;
    }

    /**
     * Ã‰TAPE 3 (Sous-tÃ¢che): Traite un texte brut pour en extraire une liste de racines filtrÃ©es.
     */
    private static List<String> processText(String text, IStemmer stemmer, Set<String> stopWords) {
        System.out.println("  1. Racinisation du texte (stemming)...");
        List<WordStemmerAnalysis> analyses = stemmer.stem(text);
        List<String> docStems = new ArrayList<>();
        System.out.println("  2. Extraction des morphÃ¨mes et filtrage...");

        for (WordStemmerAnalysis wordAnalysis : analyses) {
            var analysisList = wordAnalysis.getListStemmerAnalysis();
            if (analysisList != null && !analysisList.isEmpty()) {
                StemmerAnalysis firstAnalysis = analysisList.get(0);
                String stem = firstAnalysis.getMorpheme();

                if (isValidStem(stem, stopWords)) {
                    docStems.add(stem);
                }
            }
        }
        return docStems;
    }

    /**
     * Ã‰TAPE 3 (Sous-tÃ¢che): DÃ©finit les rÃ¨gles de filtrage pour une racine (stem).
     */
    private static boolean isValidStem(String stem, Set<String> stopWords) {
        return stem != null &&           // Ne doit pas Ãªtre nul
                !stem.isBlank() &&        // Ne doit pas Ãªtre vide
                !stopWords.contains(stem) && // Ne doit pas Ãªtre un mot vide
                !stem.matches("\\d+") &&  // Ne doit pas Ãªtre un nombre
                stem.length() > 1;        // Ne doit pas Ãªtre une seule lettre
    }

    /**
     * Ã‰TAPE 4: Calcule la carte TF pour tous les documents.
     */
    private static Map<String, Map<String, Double>> computeTfMap(Map<String, Map<String, Long>> occurrenceMap) {
        Map<String, Map<String, Double>> tfMap = new LinkedHashMap<>();
        for (var entry : occurrenceMap.entrySet()) {
            // APPEL Ã€ LA CLASSE UTILS
            tfMap.put(entry.getKey(), TfIdfUtils.computeTF(entry.getValue()));
        }
        System.out.println("  âœ… Scores TF calculÃ©s.");
        return tfMap;
    }

    /**
     * Ã‰TAPE 4: Affiche les aperÃ§us TF et IDF dans la console.
     */
    private static void printTfIdfApercus(Map<String, Map<String, Double>> tfMap, Map<String, Double> idfMap) {
        System.out.println("\n--- APERÃ‡U SCORES TF ---");
        tfMap.forEach((doc, map) -> {
            System.out.println("  Document: \"" + doc + "\"");
            map.entrySet().stream().limit(5).forEach((entry) -> {
                System.out.println(String.format("    \"%s\": %.8f", entry.getKey(), entry.getValue()));
            });
            System.out.println("    ...");
        });

        System.out.println("\n--- APERÃ‡U SCORES IDF ---");
        idfMap.entrySet().stream().limit(10).forEach((entry) -> {
            System.out.println(String.format("  \"%s\": %.8f", entry.getKey(), entry.getValue()));
        });
        System.out.println("  ...");
    }

    /**
     * Ã‰TAPE 6: Convertit la carte TF-IDF finale en JSON et la sauvegarde dans un fichier.
     */
    private static void saveResultsAsJson(Map<String, Map<String, Double>> tfIdfMap, String outputPath) throws IOException {
        System.out.println("  -> GÃ©nÃ©ration de la chaÃ®ne JSON...");
        // APPEL Ã€ LA CLASSE UTILS
        String jsonOutput = TfIdfUtils.convertMapToJson(tfIdfMap);

        System.out.println("  -> Ã‰criture du fichier vers: " + outputPath);
        Files.writeString(Paths.get(outputPath), jsonOutput, StandardCharsets.UTF_8);

        System.out.println("âœ… Fichier JSON sauvegardÃ© avec succÃ¨s !");

        System.out.println("\n--- APERÃ‡U DE LA MATRICE TF-IDF (JSON) ---");
        System.out.println(jsonOutput);
    }
}